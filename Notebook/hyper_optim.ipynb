{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\Documents\\UBA\\machine learning 2\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso\n",
    "from scipy import stats\n",
    "import optuna\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data from the specified input_path.\n",
    "\n",
    "    :return Dataframe \n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    items = os.listdir(input_path)\n",
    "\n",
    "    for dataset in items:\n",
    "        if dataset.lower().startswith('train'):\n",
    "            train_path = dataset\n",
    "\n",
    "    data_train = pd.read_csv(input_path + train_path, index_col=0)\n",
    "\n",
    "    return data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')\n",
    "x_data = data_frame.drop(columns='Item_Outlet_Sales')\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "x_data, data_frame['Item_Outlet_Sales'], test_size=0.3, random_state=28)\n",
    "\n",
    "def objective_linear(trial: optuna.Trial):\n",
    " \n",
    "    alpha = trial.suggest_float('alpha',0.0,10)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.05,1.0)\n",
    "    intercept = trial.suggest_categorical('fit_intercept',[True,False])\n",
    "    tol = trial.suggest_float('tol',0.001,0.01,log = True)\n",
    "    selection = trial.suggest_categorical(\"selection\",['random','cyclic'])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\",\"cholesky\", \"lsqr\", \"saga\", \"sag\"])\n",
    "    elastic_net = Lasso(alpha = alpha,\n",
    "                              fit_intercept = intercept ,\n",
    "                              tol=tol,\n",
    "                             selection=selection\n",
    "                             ,random_state=0)\n",
    "    \n",
    "    elastic_net.fit(x_train, y_train)\n",
    "    \n",
    "    pred = elastic_net.predict(x_val)\n",
    "    mse_train = metrics.mean_squared_error(y_train, elastic_net.predict(x_train))\n",
    "    r2_train = elastic_net.score(x_train, y_train)\n",
    "\n",
    "    # print('TRAINING: RMSE: %f - R2: %f', mse_train**0.5 ,r2_train)\n",
    "\n",
    "    mse_val = metrics.mean_squared_error(y_val, pred)\n",
    "    r2_val = elastic_net.score(x_val, y_val)\n",
    "    # print('VALIDATION: RMSE: %f - R2: %f' ,mse_val**0.5  ,r2_val)\n",
    "\n",
    "\n",
    "    # # Constante del elastic_net\n",
    "    # print('\\nIntersection: %s', elastic_net.intercept_)\n",
    "\n",
    "    # Coeficientes del elastic_net\n",
    "    return r2_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')\n",
    "x_data = data_frame.drop(columns='Item_Outlet_Sales')\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "x_data, data_frame['Item_Outlet_Sales'], test_size=0.3, random_state=28)\n",
    "\n",
    "def objective_Forest(trial: optuna.Trial):\n",
    " \n",
    "    n_estimators = trial.suggest_int('n_estimators', 500, 700)\n",
    "    criterion = trial.suggest_categorical(\"criterion\", ['squared_error', 'friedman_mse'])\n",
    "    max_depth = trial.suggest_int('max_depth', 7, 11)\n",
    "    #min_samples_split = trial.suggest_int(\"min_samples_split\", 1, 2)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    \n",
    "    # Crear el clasificador con los hiperparámetros sugeridos por Optuna\n",
    "    forest = RandomForestRegressor(\n",
    "                                    n_estimators=n_estimators,\n",
    "                                    max_depth=max_depth,\n",
    "                                    #min_samples_split=min_samples_split,\n",
    "                                    #min_samples_leaf=min_samples_leaf,\n",
    "                                    criterion = criterion,\n",
    "                                    n_jobs= -1\n",
    "                                    )\n",
    "    \n",
    "    forest.fit(x_train, y_train)\n",
    "    \n",
    "    pred = forest.predict(x_val)\n",
    "    mse_train = metrics.mean_squared_error(y_train, forest.predict(x_train))\n",
    "    r2_train = forest.score(x_train, y_train)\n",
    "\n",
    "    # print('TRAINING: RMSE: %f - R2: %f', mse_train**0.5 ,r2_train)\n",
    "\n",
    "    mse_val = metrics.mean_squared_error(y_val, pred)\n",
    "    r2_val = forest.score(x_val, y_val)\n",
    "    # print('VALIDATION: RMSE: %f - R2: %f' ,mse_val**0.5  ,r2_val)\n",
    "\n",
    "\n",
    "    # # Constante del elastic_net\n",
    "    # print('\\nIntersection: %s', elastic_net.intercept_)\n",
    "\n",
    "    # Coeficientes del elastic_net\n",
    "    return r2_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')\n",
    "x_data = data_frame.drop(columns='Item_Outlet_Sales')\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "x_data, data_frame['Item_Outlet_Sales'], test_size=0.3, random_state=28)\n",
    "\n",
    "def objective_GradientBoost(trial: optuna.Trial):\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate',0.01,0.1)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 500, log=True)\n",
    "    loss = trial.suggest_categorical('loss',['squared_error', 'huber'])\n",
    "    criterion = trial.suggest_categorical(\"criterion\", ['squared_error', 'friedman_mse'])\n",
    "    max_depth = trial.suggest_int('max_depth', 6, 12)\n",
    "    #min_samples_split = trial.suggest_float(\"min_samples_split\", 0.01, 1)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    \n",
    "    # Crear el clasificador con los hiperparámetros sugeridos por Optuna\n",
    "    forest = GradientBoostingRegressor(\n",
    "        #learning_rate= learning_rate,\n",
    "        loss = loss,\n",
    "                                    n_estimators=n_estimators,\n",
    "                                    max_depth=max_depth,\n",
    "                                    #min_samples_split=min_samples_split,\n",
    "                                    #min_samples_leaf=min_samples_leaf,\n",
    "                                    criterion = 'squared_error',\n",
    "                           \n",
    "                                    )\n",
    "    \n",
    "    forest.fit(x_train, y_train)\n",
    "    \n",
    "    pred = forest.predict(x_val)\n",
    "    mse_train = metrics.mean_squared_error(y_train, forest.predict(x_train))\n",
    "    r2_train = forest.score(x_train, y_train)\n",
    "\n",
    "    # print('TRAINING: RMSE: %f - R2: %f', mse_train**0.5 ,r2_train)\n",
    "\n",
    "    mse_val = metrics.mean_squared_error(y_val, pred)\n",
    "    r2_val = forest.score(x_val, y_val)\n",
    "    # print('VALIDATION: RMSE: %f - R2: %f' ,mse_val**0.5  ,r2_val)\n",
    "\n",
    "\n",
    "    # # Constante del elastic_net\n",
    "    # print('\\nIntersection: %s', elastic_net.intercept_)\n",
    "\n",
    "    # Coeficientes del elastic_net\n",
    "    return r2_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')\n",
    "x_data = data_frame.drop(columns='Item_Outlet_Sales')\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "x_data, data_frame['Item_Outlet_Sales'], test_size=0.3, random_state=28)\n",
    "\n",
    "def objective_Tree(trial: optuna.Trial):\n",
    "    \n",
    "    #learning_rate = trial.suggest_float('learning_rate',0.2,0.5)\n",
    "    #n_estimators = trial.suggest_int('n_estimators', 100, 1000, log=True)\n",
    "    #loss = trial.suggest_categorical('loss',['squared_error', 'huber'])\n",
    "    criterion = trial.suggest_categorical(\"criterion\", ['squared_error', 'friedman_mse'])\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 22)\n",
    "    #min_samples_split = trial.suggest_float(\"min_samples_split\", 0.01, 1)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    \n",
    "    # Crear el clasificador con los hiperparámetros sugeridos por Optuna\n",
    "    forest = DecisionTreeRegressor(\n",
    "        #learning_rate= learning_rate,\n",
    "        #loss = 'linear',\n",
    "                                    #n_estimators=n_estimators,\n",
    "                                    #random_state=0\n",
    "                                    max_depth=max_depth,\n",
    "                                    #min_samples_split=min_samples_split,\n",
    "                                    #min_samples_leaf=min_samples_leaf,\n",
    "                                    criterion = criterion,\n",
    "                           \n",
    "                                    )\n",
    "    \n",
    "    forest.fit(x_train, y_train)\n",
    "    \n",
    "    pred = forest.predict(x_val)\n",
    "    mse_train = metrics.mean_squared_error(y_train, forest.predict(x_train))\n",
    "    r2_train = forest.score(x_train, y_train)\n",
    "\n",
    "    # print('TRAINING: RMSE: %f - R2: %f', mse_train**0.5 ,r2_train)\n",
    "\n",
    "    mse_val = metrics.mean_squared_error(y_val, pred)\n",
    "    r2_val = forest.score(x_val, y_val)\n",
    "    # print('VALIDATION: RMSE: %f - R2: %f' ,mse_val**0.5  ,r2_val)\n",
    "\n",
    "\n",
    "    # # Constante del elastic_net\n",
    "    # print('\\nIntersection: %s', elastic_net.intercept_)\n",
    "\n",
    "    # Coeficientes del elastic_net\n",
    "    return r2_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-14 23:23:38,673] A new study created in memory with name: no-name-2c1b67a6-fc4a-459a-a2b2-11f8520ca785\n",
      "[I 2023-06-14 23:23:38,738] Trial 0 finished with value: 0.331068356155509 and parameters: {'criterion': 'friedman_mse', 'max_depth': 17}. Best is trial 0 with value: 0.331068356155509.\n",
      "[I 2023-06-14 23:23:38,778] Trial 1 finished with value: 0.5738068251999415 and parameters: {'criterion': 'squared_error', 'max_depth': 6}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:38,841] Trial 2 finished with value: 0.37469487337720475 and parameters: {'criterion': 'friedman_mse', 'max_depth': 15}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:38,911] Trial 3 finished with value: 0.2588555981154489 and parameters: {'criterion': 'squared_error', 'max_depth': 22}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:38,945] Trial 4 finished with value: 0.5658459632875563 and parameters: {'criterion': 'squared_error', 'max_depth': 7}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:38,993] Trial 5 finished with value: 0.42853670261722887 and parameters: {'criterion': 'friedman_mse', 'max_depth': 13}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:39,044] Trial 6 finished with value: 0.37591675823776727 and parameters: {'criterion': 'squared_error', 'max_depth': 15}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:39,093] Trial 7 finished with value: 0.5177811971427864 and parameters: {'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:39,143] Trial 8 finished with value: 0.5651276277013141 and parameters: {'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 1 with value: 0.5738068251999415.\n",
      "[I 2023-06-14 23:23:39,198] Trial 9 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,247] Trial 10 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,301] Trial 11 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,378] Trial 12 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,431] Trial 13 finished with value: 0.5175988354746258 and parameters: {'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,479] Trial 14 finished with value: 0.5179358892616956 and parameters: {'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,513] Trial 15 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,560] Trial 16 finished with value: 0.5487078312272955 and parameters: {'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,634] Trial 17 finished with value: 0.2716925911671302 and parameters: {'criterion': 'friedman_mse', 'max_depth': 21}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,682] Trial 18 finished with value: 0.524780058110633 and parameters: {'criterion': 'squared_error', 'max_depth': 9}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,734] Trial 19 finished with value: 0.4618876903857879 and parameters: {'criterion': 'friedman_mse', 'max_depth': 12}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,806] Trial 20 finished with value: 0.3111094987287555 and parameters: {'criterion': 'friedman_mse', 'max_depth': 19}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,843] Trial 21 finished with value: 0.5838280614738387 and parameters: {'criterion': 'friedman_mse', 'max_depth': 5}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,883] Trial 22 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,929] Trial 23 finished with value: 0.5738068251999415 and parameters: {'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:39,972] Trial 24 finished with value: 0.5738068251999415 and parameters: {'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,013] Trial 25 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,058] Trial 26 finished with value: 0.549735743444297 and parameters: {'criterion': 'squared_error', 'max_depth': 8}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,114] Trial 27 finished with value: 0.46432160466260863 and parameters: {'criterion': 'friedman_mse', 'max_depth': 12}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,154] Trial 28 finished with value: 0.5838280614738387 and parameters: {'criterion': 'friedman_mse', 'max_depth': 5}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,199] Trial 29 finished with value: 0.5488860732515102 and parameters: {'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,239] Trial 30 finished with value: 0.5838280614738387 and parameters: {'criterion': 'friedman_mse', 'max_depth': 5}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,280] Trial 31 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,323] Trial 32 finished with value: 0.5738068251999415 and parameters: {'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,361] Trial 33 finished with value: 0.5838280614738386 and parameters: {'criterion': 'friedman_mse', 'max_depth': 5}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,401] Trial 34 finished with value: 0.5658887404232177 and parameters: {'criterion': 'squared_error', 'max_depth': 7}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,435] Trial 35 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,496] Trial 36 finished with value: 0.3528687401665638 and parameters: {'criterion': 'friedman_mse', 'max_depth': 16}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,537] Trial 37 finished with value: 0.5659090669594042 and parameters: {'criterion': 'squared_error', 'max_depth': 7}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,573] Trial 38 finished with value: 0.5838280614738387 and parameters: {'criterion': 'friedman_mse', 'max_depth': 5}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,617] Trial 39 finished with value: 0.5738583905062482 and parameters: {'criterion': 'squared_error', 'max_depth': 6}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,662] Trial 40 finished with value: 0.523636548657638 and parameters: {'criterion': 'friedman_mse', 'max_depth': 9}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,696] Trial 41 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,734] Trial 42 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,776] Trial 43 finished with value: 0.5738012193340501 and parameters: {'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,817] Trial 44 finished with value: 0.5651216952016641 and parameters: {'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,856] Trial 45 finished with value: 0.5838280614738387 and parameters: {'criterion': 'friedman_mse', 'max_depth': 5}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,895] Trial 46 finished with value: 0.5916246116950783 and parameters: {'criterion': 'friedman_mse', 'max_depth': 4}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:40,951] Trial 47 finished with value: 0.40908953893534095 and parameters: {'criterion': 'friedman_mse', 'max_depth': 14}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:41,014] Trial 48 finished with value: 0.3225723291518392 and parameters: {'criterion': 'friedman_mse', 'max_depth': 18}. Best is trial 9 with value: 0.5916246116950783.\n",
      "[I 2023-06-14 23:23:41,062] Trial 49 finished with value: 0.5241595686743263 and parameters: {'criterion': 'squared_error', 'max_depth': 9}. Best is trial 9 with value: 0.5916246116950783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: score 0.5916246116950783, params {'criterion': 'friedman_mse', 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler= sampler)\n",
    "study.optimize(objective_Tree, n_trials=50)\n",
    "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-14 22:48:08,609] A new study created in memory with name: no-name-b0963055-5da6-4ae1-83ad-d0c556176b76\n",
      "[I 2023-06-14 22:48:17,999] Trial 0 finished with value: 0.5070196618585538 and parameters: {'learning_rate': 0.04370861069626263, 'n_estimators': 462, 'loss': 'squared_error', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 0 with value: 0.5070196618585538.\n",
      "[I 2023-06-14 22:48:23,995] Trial 1 finished with value: 0.5251929049906798 and parameters: {'learning_rate': 0.08795585311974417, 'n_estimators': 263, 'loss': 'squared_error', 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:48:43,858] Trial 2 finished with value: 0.5007350223675381 and parameters: {'learning_rate': 0.02636424704863906, 'n_estimators': 134, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 10}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:49:00,014] Trial 3 finished with value: 0.5134667948047928 and parameters: {'learning_rate': 0.022554447458683766, 'n_estimators': 160, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 9}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:49:04,055] Trial 4 finished with value: 0.481076178228243 and parameters: {'learning_rate': 0.06331731119758383, 'n_estimators': 107, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 12}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:49:20,839] Trial 5 finished with value: 0.514415397435342 and parameters: {'learning_rate': 0.0827557613304815, 'n_estimators': 163, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 9}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:49:56,867] Trial 6 finished with value: 0.4810634682454038 and parameters: {'learning_rate': 0.013094966900369657, 'n_estimators': 432, 'loss': 'huber', 'criterion': 'friedman_mse', 'max_depth': 9}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:51:18,562] Trial 7 finished with value: 0.44434009837205557 and parameters: {'learning_rate': 0.026636900997297437, 'n_estimators': 477, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 12}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:51:44,207] Trial 8 finished with value: 0.4928337039298407 and parameters: {'learning_rate': 0.017964325184672756, 'n_estimators': 137, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 11}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:51:50,034] Trial 9 finished with value: 0.47127738336616676 and parameters: {'learning_rate': 0.04210779940242304, 'n_estimators': 157, 'loss': 'squared_error', 'criterion': 'squared_error', 'max_depth': 12}. Best is trial 1 with value: 0.5251929049906798.\n",
      "[I 2023-06-14 22:51:55,455] Trial 10 finished with value: 0.5365118952981534 and parameters: {'learning_rate': 0.09809605894384237, 'n_estimators': 287, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:00,942] Trial 11 finished with value: 0.5339872154806096 and parameters: {'learning_rate': 0.09424097081084175, 'n_estimators': 290, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:07,063] Trial 12 finished with value: 0.5203421125075784 and parameters: {'learning_rate': 0.09931461007033648, 'n_estimators': 279, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:13,454] Trial 13 finished with value: 0.5249452177754483 and parameters: {'learning_rate': 0.09697459188181978, 'n_estimators': 335, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:18,024] Trial 14 finished with value: 0.5352260611771553 and parameters: {'learning_rate': 0.07763336282405137, 'n_estimators': 208, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:23,295] Trial 15 finished with value: 0.5115185311537992 and parameters: {'learning_rate': 0.0777901954623825, 'n_estimators': 212, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:28,100] Trial 16 finished with value: 0.5313259747298897 and parameters: {'learning_rate': 0.07578201720730748, 'n_estimators': 218, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:33,121] Trial 17 finished with value: 0.5177126393019862 and parameters: {'learning_rate': 0.07008082596012055, 'n_estimators': 201, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:41,829] Trial 18 finished with value: 0.4907169119684067 and parameters: {'learning_rate': 0.08663468489850422, 'n_estimators': 350, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:47,416] Trial 19 finished with value: 0.5281233181073848 and parameters: {'learning_rate': 0.06931257443280767, 'n_estimators': 254, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 10 with value: 0.5365118952981534.\n",
      "[I 2023-06-14 22:52:51,004] Trial 20 finished with value: 0.5540207521187176 and parameters: {'learning_rate': 0.0894521648082216, 'n_estimators': 188, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 20 with value: 0.5540207521187176.\n",
      "[I 2023-06-14 22:52:54,586] Trial 21 finished with value: 0.5531138155301001 and parameters: {'learning_rate': 0.08809154225175217, 'n_estimators': 188, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 20 with value: 0.5540207521187176.\n",
      "[I 2023-06-14 22:52:58,123] Trial 22 finished with value: 0.5542205146855627 and parameters: {'learning_rate': 0.09063156586603174, 'n_estimators': 186, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 22 with value: 0.5542205146855627.\n",
      "[I 2023-06-14 22:53:01,546] Trial 23 finished with value: 0.5542543505199057 and parameters: {'learning_rate': 0.08792835463369203, 'n_estimators': 179, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 23 with value: 0.5542543505199057.\n",
      "[I 2023-06-14 22:53:04,995] Trial 24 finished with value: 0.5546881928668707 and parameters: {'learning_rate': 0.09162400297671734, 'n_estimators': 181, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 24 with value: 0.5546881928668707.\n",
      "[I 2023-06-14 22:53:10,993] Trial 25 finished with value: 0.5065625016435658 and parameters: {'learning_rate': 0.09237086705316158, 'n_estimators': 239, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 24 with value: 0.5546881928668707.\n",
      "[I 2023-06-14 22:53:15,174] Trial 26 finished with value: 0.5389203939939788 and parameters: {'learning_rate': 0.08274776554731843, 'n_estimators': 188, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 24 with value: 0.5546881928668707.\n",
      "[I 2023-06-14 22:53:22,451] Trial 27 finished with value: 0.4858139760663136 and parameters: {'learning_rate': 0.09334204717208074, 'n_estimators': 234, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 24 with value: 0.5546881928668707.\n",
      "[I 2023-06-14 22:53:25,806] Trial 28 finished with value: 0.5551912652838762 and parameters: {'learning_rate': 0.08363188226695972, 'n_estimators': 175, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 28 with value: 0.5551912652838762.\n",
      "[I 2023-06-14 22:53:29,048] Trial 29 finished with value: 0.5583872900834705 and parameters: {'learning_rate': 0.08232223451143762, 'n_estimators': 167, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 29 with value: 0.5583872900834705.\n",
      "[I 2023-06-14 22:53:32,192] Trial 30 finished with value: 0.5501930641206194 and parameters: {'learning_rate': 0.05757579821620378, 'n_estimators': 142, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 29 with value: 0.5583872900834705.\n",
      "[I 2023-06-14 22:53:35,383] Trial 31 finished with value: 0.5568304254341376 and parameters: {'learning_rate': 0.08275730887844164, 'n_estimators': 168, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 29 with value: 0.5583872900834705.\n",
      "[I 2023-06-14 22:53:38,989] Trial 32 finished with value: 0.5564130775432752 and parameters: {'learning_rate': 0.08132668981171204, 'n_estimators': 168, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 29 with value: 0.5583872900834705.\n",
      "[I 2023-06-14 22:53:42,108] Trial 33 finished with value: 0.5551460106876782 and parameters: {'learning_rate': 0.07236406483083965, 'n_estimators': 123, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 29 with value: 0.5583872900834705.\n",
      "[I 2023-06-14 22:53:45,474] Trial 34 finished with value: 0.5599781956317433 and parameters: {'learning_rate': 0.0646638438644724, 'n_estimators': 162, 'loss': 'squared_error', 'criterion': 'friedman_mse', 'max_depth': 6}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:53:58,050] Trial 35 finished with value: 0.5404418920862114 and parameters: {'learning_rate': 0.06622960676191827, 'n_estimators': 151, 'loss': 'huber', 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:01,604] Trial 36 finished with value: 0.5572430290419279 and parameters: {'learning_rate': 0.060948823015870526, 'n_estimators': 166, 'loss': 'squared_error', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:26,263] Trial 37 finished with value: 0.4963609314540933 and parameters: {'learning_rate': 0.061185711678523616, 'n_estimators': 152, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 10}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:29,484] Trial 38 finished with value: 0.5376792317510115 and parameters: {'learning_rate': 0.054426209148965995, 'n_estimators': 127, 'loss': 'squared_error', 'criterion': 'squared_error', 'max_depth': 8}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:36,192] Trial 39 finished with value: 0.5576534352050057 and parameters: {'learning_rate': 0.07334890982401476, 'n_estimators': 164, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:44,618] Trial 40 finished with value: 0.5426849750352954 and parameters: {'learning_rate': 0.07422629609203554, 'n_estimators': 145, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:50,910] Trial 41 finished with value: 0.5589767583508932 and parameters: {'learning_rate': 0.0659037956689268, 'n_estimators': 155, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:54:57,379] Trial 42 finished with value: 0.5561984605912318 and parameters: {'learning_rate': 0.06298154093626207, 'n_estimators': 158, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 34 with value: 0.5599781956317433.\n",
      "[I 2023-06-14 22:55:03,137] Trial 43 finished with value: 0.5615875277185266 and parameters: {'learning_rate': 0.06693432512211311, 'n_estimators': 136, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 43 with value: 0.5615875277185266.\n",
      "[I 2023-06-14 22:55:08,647] Trial 44 finished with value: 0.5615676948152886 and parameters: {'learning_rate': 0.06764358872569284, 'n_estimators': 133, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 43 with value: 0.5615875277185266.\n",
      "[I 2023-06-14 22:55:30,802] Trial 45 finished with value: 0.49581549256626656 and parameters: {'learning_rate': 0.06745555360489026, 'n_estimators': 114, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 11}. Best is trial 43 with value: 0.5615875277185266.\n",
      "[I 2023-06-14 22:55:38,238] Trial 46 finished with value: 0.5476238025789052 and parameters: {'learning_rate': 0.05274506448878622, 'n_estimators': 134, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 43 with value: 0.5615875277185266.\n",
      "[I 2023-06-14 22:55:42,281] Trial 47 finished with value: 0.5672137492715772 and parameters: {'learning_rate': 0.06548614175086606, 'n_estimators': 101, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}. Best is trial 47 with value: 0.5672137492715772.\n",
      "[I 2023-06-14 22:55:53,336] Trial 48 finished with value: 0.5247667004543612 and parameters: {'learning_rate': 0.06436681149161384, 'n_estimators': 104, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 9}. Best is trial 47 with value: 0.5672137492715772.\n",
      "[I 2023-06-14 22:55:59,013] Trial 49 finished with value: 0.5484342106763762 and parameters: {'learning_rate': 0.06912588548935387, 'n_estimators': 100, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 47 with value: 0.5672137492715772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: score 0.5672137492715772, params {'learning_rate': 0.06548614175086606, 'n_estimators': 101, 'loss': 'huber', 'criterion': 'squared_error', 'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler= sampler)\n",
    "study.optimize(objective_GradientBoost, n_trials=50)\n",
    "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-14 22:15:59,236] A new study created in memory with name: no-name-5b274e0a-3175-42bd-b835-a0d246fe0290\n",
      "[I 2023-06-14 22:15:59,293] Trial 0 finished with value: 0.5526148537067066 and parameters: {'alpha': 3.745401188473625, 'l1_ratio': 0.9531785910894204, 'fit_intercept': True, 'tol': 0.001432249371823025, 'selection': 'random', 'solver': 'saga'}. Best is trial 0 with value: 0.5526148537067066.\n",
      "[I 2023-06-14 22:15:59,353] Trial 1 finished with value: 0.5526833194449305 and parameters: {'alpha': 2.1233911067827616, 'l1_ratio': 0.2227337188467456, 'fit_intercept': False, 'tol': 0.003347776308515932, 'selection': 'random', 'solver': 'sag'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,380] Trial 2 finished with value: 0.5526266383563246 and parameters: {'alpha': 1.9967378215835974, 'l1_ratio': 0.538522716492931, 'fit_intercept': True, 'tol': 0.004050837781329677, 'selection': 'random', 'solver': 'svd'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,409] Trial 3 finished with value: 0.5525840791456467 and parameters: {'alpha': 4.4015249373960135, 'l1_ratio': 0.16593632310253986, 'fit_intercept': True, 'tol': 0.008115595675970505, 'selection': 'cyclic', 'solver': 'saga'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,463] Trial 4 finished with value: 0.5523633396229115 and parameters: {'alpha': 9.394989415641891, 'l1_ratio': 0.9000859829062664, 'fit_intercept': False, 'tol': 0.0012260057359187524, 'selection': 'random', 'solver': 'lsqr'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,493] Trial 5 finished with value: 0.552634333111194 and parameters: {'alpha': 5.426960831582485, 'l1_ratio': 0.18387801372602453, 'fit_intercept': True, 'tol': 0.009702573394120726, 'selection': 'random', 'solver': 'svd'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,525] Trial 6 finished with value: 0.5526095680242107 and parameters: {'alpha': 3.5846572854427263, 'l1_ratio': 0.1600756065488732, 'fit_intercept': True, 'tol': 0.0021423874956449057, 'selection': 'cyclic', 'solver': 'lsqr'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,583] Trial 7 finished with value: 0.5526323183833644 and parameters: {'alpha': 7.13244787222995, 'l1_ratio': 0.7727457961860525, 'fit_intercept': False, 'tol': 0.003117422003004632, 'selection': 'random', 'solver': 'lsqr'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,616] Trial 8 finished with value: 0.5523109726314839 and parameters: {'alpha': 9.07566473926093, 'l1_ratio': 0.28682761769143117, 'fit_intercept': False, 'tol': 0.0016935505549297925, 'selection': 'cyclic', 'solver': 'svd'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,675] Trial 9 finished with value: 0.5526584371669016 and parameters: {'alpha': 1.8657005888603584, 'l1_ratio': 0.8979310485654789, 'fit_intercept': False, 'tol': 0.00787211264452507, 'selection': 'random', 'solver': 'lsqr'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,769] Trial 10 finished with value: 0.5526479857835954 and parameters: {'alpha': 0.25068237615033206, 'l1_ratio': 0.3955412899405495, 'fit_intercept': False, 'tol': 0.0010422259339479673, 'selection': 'cyclic', 'solver': 'sag'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,857] Trial 11 finished with value: 0.5526148706551789 and parameters: {'alpha': 1.366445845050833, 'l1_ratio': 0.6366165362355005, 'fit_intercept': False, 'tol': 0.005475270056465763, 'selection': 'random', 'solver': 'cholesky'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:15:59,957] Trial 12 finished with value: 0.55268255024794 and parameters: {'alpha': 2.2261697040856316, 'l1_ratio': 0.42014057789195064, 'fit_intercept': False, 'tol': 0.006110472175729072, 'selection': 'random', 'solver': 'sag'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:16:00,101] Trial 13 finished with value: 0.5526291262908369 and parameters: {'alpha': 0.09221180959262165, 'l1_ratio': 0.3428499176875628, 'fit_intercept': False, 'tol': 0.0048925649656201605, 'selection': 'random', 'solver': 'sag'}. Best is trial 1 with value: 0.5526833194449305.\n",
      "[I 2023-06-14 22:16:00,183] Trial 14 finished with value: 0.5527150586189875 and parameters: {'alpha': 3.4014903430665915, 'l1_ratio': 0.08190659068774847, 'fit_intercept': False, 'tol': 0.00291149419826145, 'selection': 'random', 'solver': 'sag'}. Best is trial 14 with value: 0.5527150586189875.\n",
      "[I 2023-06-14 22:16:00,267] Trial 15 finished with value: 0.5527146022492864 and parameters: {'alpha': 3.1296815065620205, 'l1_ratio': 0.052512359821522005, 'fit_intercept': False, 'tol': 0.0027502100301763704, 'selection': 'random', 'solver': 'auto'}. Best is trial 14 with value: 0.5527150586189875.\n",
      "[I 2023-06-14 22:16:00,340] Trial 16 finished with value: 0.5527592743884324 and parameters: {'alpha': 5.509311456208669, 'l1_ratio': 0.05628693435050005, 'fit_intercept': False, 'tol': 0.002414825596640504, 'selection': 'random', 'solver': 'auto'}. Best is trial 16 with value: 0.5527592743884324.\n",
      "[I 2023-06-14 22:16:00,424] Trial 17 finished with value: 0.5527567109523677 and parameters: {'alpha': 5.587985830433144, 'l1_ratio': 0.11190294354416137, 'fit_intercept': False, 'tol': 0.0021470339274248483, 'selection': 'random', 'solver': 'auto'}. Best is trial 16 with value: 0.5527592743884324.\n",
      "[I 2023-06-14 22:16:00,481] Trial 18 finished with value: 0.5526444005429269 and parameters: {'alpha': 5.900217920778856, 'l1_ratio': 0.06445562199298692, 'fit_intercept': False, 'tol': 0.0021247123685506643, 'selection': 'cyclic', 'solver': 'auto'}. Best is trial 16 with value: 0.5527592743884324.\n",
      "[I 2023-06-14 22:16:00,557] Trial 19 finished with value: 0.5527036604267126 and parameters: {'alpha': 6.451318008091349, 'l1_ratio': 0.2770092439939085, 'fit_intercept': False, 'tol': 0.0021711852974702363, 'selection': 'random', 'solver': 'auto'}. Best is trial 16 with value: 0.5527592743884324.\n",
      "[I 2023-06-14 22:16:00,627] Trial 20 finished with value: 0.5527632320488328 and parameters: {'alpha': 4.97429377341607, 'l1_ratio': 0.1420577847140361, 'fit_intercept': False, 'tol': 0.0016696595972022115, 'selection': 'random', 'solver': 'auto'}. Best is trial 20 with value: 0.5527632320488328.\n",
      "[I 2023-06-14 22:16:00,704] Trial 21 finished with value: 0.5527634823338312 and parameters: {'alpha': 4.948877133223232, 'l1_ratio': 0.1307397151675742, 'fit_intercept': False, 'tol': 0.001672018448663197, 'selection': 'random', 'solver': 'auto'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:00,784] Trial 22 finished with value: 0.5527634484014887 and parameters: {'alpha': 4.569301851107029, 'l1_ratio': 0.2004131133070588, 'fit_intercept': False, 'tol': 0.001578153400904767, 'selection': 'random', 'solver': 'auto'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:00,853] Trial 23 finished with value: 0.5527622581828793 and parameters: {'alpha': 4.472737108448975, 'l1_ratio': 0.23459765338553368, 'fit_intercept': False, 'tol': 0.0016253460900098253, 'selection': 'random', 'solver': 'auto'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:00,923] Trial 24 finished with value: 0.5527633386434647 and parameters: {'alpha': 4.963864889389766, 'l1_ratio': 0.1493209952295113, 'fit_intercept': False, 'tol': 0.0016965060451790688, 'selection': 'random', 'solver': 'auto'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:00,999] Trial 25 finished with value: 0.5526758646960439 and parameters: {'alpha': 6.621334458856999, 'l1_ratio': 0.2840953605363563, 'fit_intercept': False, 'tol': 0.0010115468511047905, 'selection': 'random', 'solver': 'cholesky'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:01,050] Trial 26 finished with value: 0.5525863442873638 and parameters: {'alpha': 4.365680672524999, 'l1_ratio': 0.2158333826997813, 'fit_intercept': True, 'tol': 0.0012976319032314962, 'selection': 'cyclic', 'solver': 'auto'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:01,151] Trial 27 finished with value: 0.5525859685459888 and parameters: {'alpha': 7.58234032343921, 'l1_ratio': 0.1350597151796603, 'fit_intercept': False, 'tol': 0.0018094948339371691, 'selection': 'random', 'solver': 'auto'}. Best is trial 21 with value: 0.5527634823338312.\n",
      "[I 2023-06-14 22:16:01,242] Trial 28 finished with value: 0.5527641871518649 and parameters: {'alpha': 4.823849222358638, 'l1_ratio': 0.3394492779826328, 'fit_intercept': False, 'tol': 0.0014079576507788653, 'selection': 'random', 'solver': 'auto'}. Best is trial 28 with value: 0.5527641871518649.\n",
      "[I 2023-06-14 22:16:01,301] Trial 29 finished with value: 0.5526050639030649 and parameters: {'alpha': 3.925345722809178, 'l1_ratio': 0.45073125671974895, 'fit_intercept': True, 'tol': 0.001377001884312932, 'selection': 'random', 'solver': 'saga'}. Best is trial 28 with value: 0.5527641871518649.\n",
      "[I 2023-06-14 22:16:01,380] Trial 30 finished with value: 0.5527080902657373 and parameters: {'alpha': 2.7909956470796935, 'l1_ratio': 0.3400480256347614, 'fit_intercept': False, 'tol': 0.0014356366841954313, 'selection': 'random', 'solver': 'auto'}. Best is trial 28 with value: 0.5527641871518649.\n",
      "[I 2023-06-14 22:16:01,451] Trial 31 finished with value: 0.5527641582618175 and parameters: {'alpha': 4.841028957829588, 'l1_ratio': 0.2048306583210884, 'fit_intercept': False, 'tol': 0.0011654412812553178, 'selection': 'random', 'solver': 'auto'}. Best is trial 28 with value: 0.5527641871518649.\n",
      "[I 2023-06-14 22:16:01,523] Trial 32 finished with value: 0.5527425849987757 and parameters: {'alpha': 4.232904334317377, 'l1_ratio': 0.22301192687028865, 'fit_intercept': False, 'tol': 0.001138031699650039, 'selection': 'random', 'solver': 'auto'}. Best is trial 28 with value: 0.5527641871518649.\n",
      "[I 2023-06-14 22:16:01,603] Trial 33 finished with value: 0.5527297366510141 and parameters: {'alpha': 3.7290270305173463, 'l1_ratio': 0.24998915567582325, 'fit_intercept': False, 'tol': 0.0014276719960745288, 'selection': 'random', 'solver': 'auto'}. Best is trial 28 with value: 0.5527641871518649.\n",
      "[I 2023-06-14 22:16:01,685] Trial 34 finished with value: 0.5527643308423961 and parameters: {'alpha': 4.733483285868233, 'l1_ratio': 0.11807129207335114, 'fit_intercept': False, 'tol': 0.0011566804037886967, 'selection': 'random', 'solver': 'saga'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:01,742] Trial 35 finished with value: 0.5525716196602157 and parameters: {'alpha': 4.919683442015927, 'l1_ratio': 0.09941996282846868, 'fit_intercept': True, 'tol': 0.0012024672454257475, 'selection': 'random', 'solver': 'saga'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:01,824] Trial 36 finished with value: 0.5527348428770517 and parameters: {'alpha': 3.8836265592182957, 'l1_ratio': 0.19292014604923008, 'fit_intercept': False, 'tol': 0.0012425210622285206, 'selection': 'random', 'solver': 'saga'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:01,873] Trial 37 finished with value: 0.5524949730137982 and parameters: {'alpha': 5.832161533076351, 'l1_ratio': 0.13451507134182433, 'fit_intercept': True, 'tol': 0.0010986181295828776, 'selection': 'cyclic', 'solver': 'saga'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:01,940] Trial 38 finished with value: 0.5527600975971287 and parameters: {'alpha': 5.181645492388698, 'l1_ratio': 0.30265794820110303, 'fit_intercept': False, 'tol': 0.0010072796448703668, 'selection': 'random', 'solver': 'saga'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,012] Trial 39 finished with value: 0.5527403555074535 and parameters: {'alpha': 4.12684629959541, 'l1_ratio': 0.17369953680579078, 'fit_intercept': False, 'tol': 0.0012281177911832252, 'selection': 'random', 'solver': 'svd'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,064] Trial 40 finished with value: 0.5525659446206888 and parameters: {'alpha': 4.726755833216962, 'l1_ratio': 0.09856163847605949, 'fit_intercept': True, 'tol': 0.001462507637007968, 'selection': 'cyclic', 'solver': 'cholesky'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,135] Trial 41 finished with value: 0.5527640925405932 and parameters: {'alpha': 4.646760239168435, 'l1_ratio': 0.1927898955517105, 'fit_intercept': False, 'tol': 0.0011444488390274646, 'selection': 'random', 'solver': 'auto'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,204] Trial 42 finished with value: 0.5527609079341007 and parameters: {'alpha': 5.133858492467201, 'l1_ratio': 0.18000989556962493, 'fit_intercept': False, 'tol': 0.0011406900538512186, 'selection': 'random', 'solver': 'auto'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,277] Trial 43 finished with value: 0.5527628799314741 and parameters: {'alpha': 4.521661062175299, 'l1_ratio': 0.24335199000930263, 'fit_intercept': False, 'tol': 0.001336160280141338, 'selection': 'random', 'solver': 'lsqr'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,342] Trial 44 finished with value: 0.5527396769067969 and parameters: {'alpha': 5.9838352206069585, 'l1_ratio': 0.11474889007686634, 'fit_intercept': False, 'tol': 0.001110568452541696, 'selection': 'random', 'solver': 'svd'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,411] Trial 45 finished with value: 0.5527546387733182 and parameters: {'alpha': 5.414578996631376, 'l1_ratio': 0.18012650478366596, 'fit_intercept': False, 'tol': 0.0012718152820768956, 'selection': 'random', 'solver': 'saga'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,491] Trial 46 finished with value: 0.5527144883404169 and parameters: {'alpha': 3.472384973219763, 'l1_ratio': 0.25805674691464, 'fit_intercept': False, 'tol': 0.0019108755811862443, 'selection': 'random', 'solver': 'auto'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,564] Trial 47 finished with value: 0.5527414756457403 and parameters: {'alpha': 4.176606462235665, 'l1_ratio': 0.30428936458600725, 'fit_intercept': False, 'tol': 0.0015273824218029295, 'selection': 'random', 'solver': 'lsqr'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,652] Trial 48 finished with value: 0.5527637120654335 and parameters: {'alpha': 4.596029047359139, 'l1_ratio': 0.33972985483090257, 'fit_intercept': False, 'tol': 0.0013835416290316335, 'selection': 'random', 'solver': 'cholesky'}. Best is trial 34 with value: 0.5527643308423961.\n",
      "[I 2023-06-14 22:16:02,744] Trial 49 finished with value: 0.5526944412794725 and parameters: {'alpha': 3.093991288224202, 'l1_ratio': 0.35899451106593316, 'fit_intercept': False, 'tol': 0.001347517638508807, 'selection': 'cyclic', 'solver': 'cholesky'}. Best is trial 34 with value: 0.5527643308423961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: score 0.5527643308423961, params {'alpha': 4.733483285868233, 'l1_ratio': 0.11807129207335114, 'fit_intercept': False, 'tol': 0.0011566804037886967, 'selection': 'random', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler= sampler)\n",
    "study.optimize(objective_linear, n_trials=50)\n",
    "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-14 23:42:47,103] A new study created in memory with name: no-name-5339f4c6-4a94-4491-9b57-265c8445a698\n",
      "[I 2023-06-14 23:42:51,101] Trial 0 finished with value: 0.591258982113195 and parameters: {'n_estimators': 575, 'criterion': 'squared_error', 'max_depth': 9}. Best is trial 0 with value: 0.591258982113195.\n",
      "[I 2023-06-14 23:42:55,066] Trial 1 finished with value: 0.5847832610742116 and parameters: {'n_estimators': 531, 'criterion': 'squared_error', 'max_depth': 11}. Best is trial 0 with value: 0.591258982113195.\n",
      "[I 2023-06-14 23:42:59,747] Trial 2 finished with value: 0.5849003162320623 and parameters: {'n_estimators': 620, 'criterion': 'squared_error', 'max_depth': 11}. Best is trial 0 with value: 0.591258982113195.\n",
      "[I 2023-06-14 23:43:03,579] Trial 3 finished with value: 0.5922074523533456 and parameters: {'n_estimators': 667, 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 3 with value: 0.5922074523533456.\n",
      "[I 2023-06-14 23:43:07,215] Trial 4 finished with value: 0.5913337059569794 and parameters: {'n_estimators': 561, 'criterion': 'squared_error', 'max_depth': 8}. Best is trial 3 with value: 0.5922074523533456.\n",
      "[I 2023-06-14 23:43:11,077] Trial 5 finished with value: 0.5911342766204248 and parameters: {'n_estimators': 622, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 3 with value: 0.5922074523533456.\n",
      "[I 2023-06-14 23:43:15,096] Trial 6 finished with value: 0.5898922157114068 and parameters: {'n_estimators': 591, 'criterion': 'squared_error', 'max_depth': 9}. Best is trial 3 with value: 0.5922074523533456.\n",
      "[I 2023-06-14 23:43:18,600] Trial 7 finished with value: 0.5927033662428711 and parameters: {'n_estimators': 619, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:22,349] Trial 8 finished with value: 0.5861152374673677 and parameters: {'n_estimators': 513, 'criterion': 'friedman_mse', 'max_depth': 11}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:26,358] Trial 9 finished with value: 0.5893723443144452 and parameters: {'n_estimators': 561, 'criterion': 'friedman_mse', 'max_depth': 9}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:30,380] Trial 10 finished with value: 0.5926938201991732 and parameters: {'n_estimators': 691, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:34,193] Trial 11 finished with value: 0.5921534309344993 and parameters: {'n_estimators': 687, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:37,843] Trial 12 finished with value: 0.5922639257048392 and parameters: {'n_estimators': 651, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:42,133] Trial 13 finished with value: 0.5915364375011857 and parameters: {'n_estimators': 700, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:45,777] Trial 14 finished with value: 0.5923580682733828 and parameters: {'n_estimators': 640, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:50,340] Trial 15 finished with value: 0.587443820050475 and parameters: {'n_estimators': 675, 'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:54,041] Trial 16 finished with value: 0.5908940776203837 and parameters: {'n_estimators': 612, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:43:57,661] Trial 17 finished with value: 0.5922327310923949 and parameters: {'n_estimators': 646, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:44:01,728] Trial 18 finished with value: 0.5871599698220643 and parameters: {'n_estimators': 588, 'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:44:05,925] Trial 19 finished with value: 0.5908509702076592 and parameters: {'n_estimators': 700, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 7 with value: 0.5927033662428711.\n",
      "[I 2023-06-14 23:44:09,609] Trial 20 finished with value: 0.5927369805019561 and parameters: {'n_estimators': 662, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:13,324] Trial 21 finished with value: 0.592519124305281 and parameters: {'n_estimators': 667, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:16,997] Trial 22 finished with value: 0.591910801608645 and parameters: {'n_estimators': 635, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:21,019] Trial 23 finished with value: 0.5911479897938671 and parameters: {'n_estimators': 657, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:24,831] Trial 24 finished with value: 0.5920930338815251 and parameters: {'n_estimators': 673, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:29,184] Trial 25 finished with value: 0.5909931417919911 and parameters: {'n_estimators': 685, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:32,643] Trial 26 finished with value: 0.5923672970370708 and parameters: {'n_estimators': 605, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:37,099] Trial 27 finished with value: 0.5874893503414305 and parameters: {'n_estimators': 637, 'criterion': 'friedman_mse', 'max_depth': 10}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:41,224] Trial 28 finished with value: 0.5914627579864447 and parameters: {'n_estimators': 658, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:45,487] Trial 29 finished with value: 0.5895139104301683 and parameters: {'n_estimators': 625, 'criterion': 'squared_error', 'max_depth': 9}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:49,559] Trial 30 finished with value: 0.5924909348134227 and parameters: {'n_estimators': 683, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:53,518] Trial 31 finished with value: 0.5924467086554426 and parameters: {'n_estimators': 663, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:44:57,655] Trial 32 finished with value: 0.5927052050418566 and parameters: {'n_estimators': 687, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:01,733] Trial 33 finished with value: 0.5921887142981883 and parameters: {'n_estimators': 688, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:05,809] Trial 34 finished with value: 0.5921071806149747 and parameters: {'n_estimators': 700, 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:09,754] Trial 35 finished with value: 0.5909356031222375 and parameters: {'n_estimators': 571, 'criterion': 'squared_error', 'max_depth': 8}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:14,098] Trial 36 finished with value: 0.5922949854167473 and parameters: {'n_estimators': 676, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:19,376] Trial 37 finished with value: 0.591144445782998 and parameters: {'n_estimators': 650, 'criterion': 'squared_error', 'max_depth': 8}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:23,653] Trial 38 finished with value: 0.5918379539233621 and parameters: {'n_estimators': 629, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:27,408] Trial 39 finished with value: 0.5909852577830506 and parameters: {'n_estimators': 541, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:30,877] Trial 40 finished with value: 0.5916239293545815 and parameters: {'n_estimators': 591, 'criterion': 'squared_error', 'max_depth': 7}. Best is trial 20 with value: 0.5927369805019561.\n",
      "[I 2023-06-14 23:45:34,661] Trial 41 finished with value: 0.5929260529701437 and parameters: {'n_estimators': 661, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:45:39,056] Trial 42 finished with value: 0.5925782242830457 and parameters: {'n_estimators': 689, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:45:43,036] Trial 43 finished with value: 0.592760309675104 and parameters: {'n_estimators': 675, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:45:46,605] Trial 44 finished with value: 0.5925490496111585 and parameters: {'n_estimators': 614, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:45:52,312] Trial 45 finished with value: 0.5839960287564047 and parameters: {'n_estimators': 669, 'criterion': 'friedman_mse', 'max_depth': 11}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:45:57,224] Trial 46 finished with value: 0.5912672702777702 and parameters: {'n_estimators': 657, 'criterion': 'friedman_mse', 'max_depth': 8}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:46:01,505] Trial 47 finished with value: 0.5923027054922512 and parameters: {'n_estimators': 681, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:46:05,691] Trial 48 finished with value: 0.5928419532490143 and parameters: {'n_estimators': 646, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n",
      "[I 2023-06-14 23:46:09,770] Trial 49 finished with value: 0.5920579491071161 and parameters: {'n_estimators': 645, 'criterion': 'friedman_mse', 'max_depth': 7}. Best is trial 41 with value: 0.5929260529701437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: score 0.5929260529701437, params {'n_estimators': 661, 'criterion': 'friedman_mse', 'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler= sampler)\n",
    "study.optimize(objective_Forest, n_trials=50)\n",
    "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim, cuda, sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')\n",
    "x_data = data_frame.drop(columns='Item_Outlet_Sales')\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "x_data, data_frame['Item_Outlet_Sales'], test_size=0.3, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8523 entries, 0 to 8522\n",
      "Data columns (total 10 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Item_Weight                    8523 non-null   float64\n",
      " 1   Item_Visibility                8523 non-null   float64\n",
      " 2   Item_MRP                       8523 non-null   int64  \n",
      " 3   Outlet_Establishment_Year      8523 non-null   int64  \n",
      " 4   Outlet_Size                    8523 non-null   int64  \n",
      " 5   Outlet_Location_Type           8523 non-null   int64  \n",
      " 6   Outlet_Type_Grocery Store      8523 non-null   int64  \n",
      " 7   Outlet_Type_Supermarket Type1  8523 non-null   int64  \n",
      " 8   Outlet_Type_Supermarket Type2  8523 non-null   int64  \n",
      " 9   Outlet_Type_Supermarket Type3  8523 non-null   int64  \n",
      "dtypes: float64(2), int64(8)\n",
      "memory usage: 732.4 KB\n"
     ]
    }
   ],
   "source": [
    "x_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_data.to_numpy()\n",
    "y =  data_frame['Item_Outlet_Sales'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (x - np.min(x, axis=0)) / (np.max(x, axis=0 ) - np.min(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8055, 1135, 2338, ..., 3686,  495, 1380])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.permutation(x_norm.shape[0])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7244,)\n",
      "(1279,)\n"
     ]
    }
   ],
   "source": [
    "train_idx = idx[0:int(0.85*len(idx))]\n",
    "valid_idx = idx[int(0.85*len(idx)):]\n",
    "print(train_idx.shape)\n",
    "print(valid_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_norm[train_idx]\n",
    "train_y = y[train_idx]\n",
    "valid_x = x_norm[valid_idx]\n",
    "valid_y = y[valid_idx]\n",
    "\n",
    "n_train = train_x.shape[0]\n",
    "n_valid = valid_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "  #__init__ guarda el dataset en una variable de clase\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "\n",
    "  # __len__ define el comportamiento de la función len() sobre el objeto\n",
    "  def __len__(self):\n",
    "    return self.x.shape[0]\n",
    "\n",
    "  # __getitem__ define el comportamiento de los []\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MyDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = MyDataset(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size = 64, shuffle= True)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    # Defino la arquitectura de la red\n",
    "    super().__init__()\n",
    "    self.linear_1 = nn.Linear(in_features=10 , out_features=4, bias=True)\n",
    "    self.relu_1 = nn.ReLU()\n",
    "    self.linear_2 = nn.Linear(in_features = 4, out_features=2, bias=True)\n",
    "    self.relu_2 = nn.ReLU()\n",
    "    self.output = nn.Linear(in_features = 2, out_features= 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Defino el cálculo del paso forward\n",
    "    x = self.linear_1(x)\n",
    "    x = self.relu_1(x)\n",
    "    x = self.linear_2(x)\n",
    "    x = self.relu_2(x)\n",
    "    x = self.output(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = NNet()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(nnet.parameters(), lr=0.03, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0 | Train/Valid loss: 114942.169 / 87286.988 | Train/Valid accuracy: -1.510 / -0.910\n",
      " Epoch 1 | Train/Valid loss: 49519.800 / 35308.043 | Train/Valid accuracy: -0.085 / 0.227\n",
      " Epoch 2 | Train/Valid loss: 33489.630 / 30508.958 | Train/Valid accuracy: 0.267 / 0.332\n",
      " Epoch 3 | Train/Valid loss: 29577.831 / 26504.220 | Train/Valid accuracy: 0.360 / 0.420\n",
      " Epoch 4 | Train/Valid loss: 25648.931 / 23067.592 | Train/Valid accuracy: 0.440 / 0.495\n",
      " Epoch 5 | Train/Valid loss: 22928.744 / 20818.870 | Train/Valid accuracy: 0.499 / 0.545\n",
      " Epoch 6 | Train/Valid loss: 21173.625 / 19613.070 | Train/Valid accuracy: 0.535 / 0.571\n",
      " Epoch 7 | Train/Valid loss: 20492.301 / 19163.594 | Train/Valid accuracy: 0.551 / 0.581\n",
      " Epoch 8 | Train/Valid loss: 20321.131 / 19030.118 | Train/Valid accuracy: 0.556 / 0.584\n",
      " Epoch 9 | Train/Valid loss: 20191.713 / 19037.228 | Train/Valid accuracy: 0.558 / 0.584\n",
      " Epoch 10 | Train/Valid loss: 20218.297 / 18945.614 | Train/Valid accuracy: 0.558 / 0.586\n",
      " Epoch 11 | Train/Valid loss: 20208.071 / 18983.425 | Train/Valid accuracy: 0.559 / 0.585\n",
      " Epoch 12 | Train/Valid loss: 20178.003 / 18943.841 | Train/Valid accuracy: 0.559 / 0.586\n",
      " Epoch 13 | Train/Valid loss: 20190.842 / 18928.239 | Train/Valid accuracy: 0.559 / 0.586\n",
      " Epoch 14 | Train/Valid loss: 20232.829 / 18919.332 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 15 | Train/Valid loss: 20134.079 / 18917.756 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 16 | Train/Valid loss: 20133.622 / 18927.172 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 17 | Train/Valid loss: 20278.578 / 18952.567 | Train/Valid accuracy: 0.560 / 0.585\n",
      " Epoch 18 | Train/Valid loss: 20147.562 / 18906.735 | Train/Valid accuracy: 0.559 / 0.586\n",
      " Epoch 19 | Train/Valid loss: 20057.058 / 18973.263 | Train/Valid accuracy: 0.560 / 0.585\n",
      " Epoch 20 | Train/Valid loss: 20127.283 / 18907.280 | Train/Valid accuracy: 0.559 / 0.586\n",
      " Epoch 21 | Train/Valid loss: 20077.382 / 18975.358 | Train/Valid accuracy: 0.560 / 0.585\n",
      " Epoch 22 | Train/Valid loss: 20166.697 / 18910.770 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 23 | Train/Valid loss: 20167.234 / 18909.326 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 24 | Train/Valid loss: 20121.896 / 18900.238 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 25 | Train/Valid loss: 20074.574 / 18900.362 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 26 | Train/Valid loss: 20140.716 / 18903.844 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 27 | Train/Valid loss: 20047.590 / 18894.513 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 28 | Train/Valid loss: 20359.931 / 18898.395 | Train/Valid accuracy: 0.559 / 0.587\n",
      " Epoch 29 | Train/Valid loss: 20076.754 / 18909.971 | Train/Valid accuracy: 0.561 / 0.586\n",
      " Epoch 30 | Train/Valid loss: 20026.569 / 18957.233 | Train/Valid accuracy: 0.560 / 0.585\n",
      " Epoch 31 | Train/Valid loss: 20093.582 / 18877.183 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 32 | Train/Valid loss: 20178.464 / 18910.752 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 33 | Train/Valid loss: 20215.667 / 18870.853 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 34 | Train/Valid loss: 20092.320 / 18895.411 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 35 | Train/Valid loss: 20171.696 / 18897.567 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 36 | Train/Valid loss: 20049.388 / 18906.680 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 37 | Train/Valid loss: 20026.807 / 18898.237 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 38 | Train/Valid loss: 20144.625 / 18918.683 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 39 | Train/Valid loss: 20024.558 / 18861.041 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 40 | Train/Valid loss: 20203.056 / 18873.117 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 41 | Train/Valid loss: 20135.548 / 18861.862 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 42 | Train/Valid loss: 20411.903 / 18867.929 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 43 | Train/Valid loss: 20164.673 / 18887.514 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 44 | Train/Valid loss: 20040.372 / 18877.361 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 45 | Train/Valid loss: 20068.900 / 18867.954 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 46 | Train/Valid loss: 20021.922 / 18861.585 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 47 | Train/Valid loss: 20239.092 / 18872.710 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 48 | Train/Valid loss: 20259.530 / 18866.618 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 49 | Train/Valid loss: 20267.800 / 18858.257 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 50 | Train/Valid loss: 20052.758 / 18885.376 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 51 | Train/Valid loss: 20207.765 / 18849.952 | Train/Valid accuracy: 0.560 / 0.588\n",
      " Epoch 52 | Train/Valid loss: 20096.935 / 18857.568 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 53 | Train/Valid loss: 20130.300 / 18856.787 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 54 | Train/Valid loss: 20029.483 / 18859.571 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 55 | Train/Valid loss: 20191.084 / 18917.270 | Train/Valid accuracy: 0.561 / 0.586\n",
      " Epoch 56 | Train/Valid loss: 20225.823 / 18890.114 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 57 | Train/Valid loss: 20064.030 / 18862.804 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 58 | Train/Valid loss: 20083.207 / 18842.430 | Train/Valid accuracy: 0.560 / 0.588\n",
      " Epoch 59 | Train/Valid loss: 20081.221 / 18848.573 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 60 | Train/Valid loss: 20064.050 / 18927.287 | Train/Valid accuracy: 0.561 / 0.586\n",
      " Epoch 61 | Train/Valid loss: 20129.357 / 18922.346 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 62 | Train/Valid loss: 20010.190 / 18851.359 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 63 | Train/Valid loss: 20101.801 / 18848.330 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 64 | Train/Valid loss: 20099.023 / 18864.054 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 65 | Train/Valid loss: 20127.249 / 18858.602 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 66 | Train/Valid loss: 20037.950 / 18854.097 | Train/Valid accuracy: 0.560 / 0.588\n",
      " Epoch 67 | Train/Valid loss: 20034.592 / 18847.342 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 68 | Train/Valid loss: 20044.043 / 18850.052 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 69 | Train/Valid loss: 20070.578 / 18856.605 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 70 | Train/Valid loss: 20067.135 / 18894.857 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 71 | Train/Valid loss: 20189.419 / 18864.253 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 72 | Train/Valid loss: 20076.359 / 18862.654 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 73 | Train/Valid loss: 19985.836 / 18953.562 | Train/Valid accuracy: 0.562 / 0.585\n",
      " Epoch 74 | Train/Valid loss: 20033.356 / 18856.867 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 75 | Train/Valid loss: 20331.305 / 18853.302 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 76 | Train/Valid loss: 20096.736 / 18853.586 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 77 | Train/Valid loss: 20173.954 / 18868.384 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 78 | Train/Valid loss: 20105.679 / 18931.417 | Train/Valid accuracy: 0.561 / 0.586\n",
      " Epoch 79 | Train/Valid loss: 20125.184 / 18860.200 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 80 | Train/Valid loss: 20112.784 / 18889.996 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 81 | Train/Valid loss: 19977.764 / 18881.433 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 82 | Train/Valid loss: 20296.806 / 18888.579 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 83 | Train/Valid loss: 20159.492 / 18845.178 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 84 | Train/Valid loss: 20036.112 / 18852.359 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 85 | Train/Valid loss: 20474.262 / 18861.563 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 86 | Train/Valid loss: 20094.914 / 18846.932 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 87 | Train/Valid loss: 20169.403 / 18854.243 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 88 | Train/Valid loss: 20148.542 / 18938.499 | Train/Valid accuracy: 0.560 / 0.586\n",
      " Epoch 89 | Train/Valid loss: 20192.041 / 18854.045 | Train/Valid accuracy: 0.560 / 0.588\n",
      " Epoch 90 | Train/Valid loss: 20131.629 / 18982.479 | Train/Valid accuracy: 0.561 / 0.585\n",
      " Epoch 91 | Train/Valid loss: 20131.924 / 18872.109 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 92 | Train/Valid loss: 20104.581 / 18860.253 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 93 | Train/Valid loss: 20017.544 / 18865.266 | Train/Valid accuracy: 0.560 / 0.587\n",
      " Epoch 94 | Train/Valid loss: 20080.152 / 18854.179 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 95 | Train/Valid loss: 20100.875 / 18880.761 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 96 | Train/Valid loss: 20131.011 / 18860.328 | Train/Valid accuracy: 0.561 / 0.587\n",
      " Epoch 97 | Train/Valid loss: 20005.688 / 18968.150 | Train/Valid accuracy: 0.562 / 0.585\n",
      " Epoch 98 | Train/Valid loss: 20011.602 / 18856.470 | Train/Valid accuracy: 0.561 / 0.588\n",
      " Epoch 99 | Train/Valid loss: 20135.203 / 18841.404 | Train/Valid accuracy: 0.561 / 0.588\n"
     ]
    }
   ],
   "source": [
    "# cantidad de epochs\n",
    "epochs = 100\n",
    "\n",
    "train_loss_by_epoch=[]\n",
    "valid_loss_by_epoch=[]\n",
    "\n",
    "# Doble loop algoritmo Mini-Batch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  nnet.train(True)\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_y_hat = []\n",
    "  epoch_y = []\n",
    "  \n",
    "  for i,data in enumerate(train_dataloader):\n",
    "\n",
    "    x_batch, y_batch = data\n",
    "\n",
    "    x_batch = x_batch.to(device).float()\n",
    "    y_batch = y_batch.to(device).float().reshape(-1, 1)\n",
    "\n",
    "\n",
    "    \n",
    "    nnet_output = nnet(x_batch)\n",
    "    y_batch_hat = nnet_output\n",
    "    \n",
    "\n",
    "    loss = loss_function(nnet_output, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    epoch_y += list(y_batch.detach().cpu().numpy())\n",
    "    epoch_y_hat += list(y_batch_hat.detach().cpu().numpy())\n",
    "\n",
    "    epoch_loss = epoch_loss + loss.item()\n",
    "\n",
    "  # Calculo la media de la loss\n",
    "  epoch_loss = epoch_loss / n_train\n",
    "  # Almaceno la loss de la epoch para graficar\n",
    "  train_loss_by_epoch.append(epoch_loss)\n",
    "  # Cálculo la métrica de la epoch\n",
    "  accuracy = metrics.r2_score(epoch_y,  epoch_y_hat)\n",
    "\n",
    "  ############################################\n",
    "  ## Validación\n",
    "  ############################################\n",
    "  # Desactivo el cálculo de gradiente para validación\n",
    "  nnet.train(False)\n",
    "\n",
    "  valid_epoch_loss = 0\n",
    "  valid_epoch_y_hat = []\n",
    "  valid_epoch_y = []\n",
    "\n",
    "  for i,data in enumerate(valid_dataloader):\n",
    "    # Obtengo los datos del batch de validación\n",
    "    x_batch, y_batch = data\n",
    "    # Copio el batch al dispositivo donde entreno la red neuronal\n",
    "    x_batch = x_batch.to(device).float()\n",
    "    y_batch = y_batch.to(device).float().reshape(-1, 1)\n",
    "\n",
    "    # Paso forward\n",
    "    nnet_output = nnet(x_batch)\n",
    "    y_batch_hat = nnet_output\n",
    "    # Calculo el loss\n",
    "    loss = loss_function(nnet_output, y_batch)\n",
    "\n",
    "    # En validación no hago backpropagation!!\n",
    "\n",
    "    # Almaceno los valores reales y mis predicciones para cálcular las métricas\n",
    "    valid_epoch_y += list(y_batch.detach().cpu().numpy())\n",
    "    valid_epoch_y_hat += list(y_batch_hat.detach().cpu().numpy())\n",
    "    # Acumulo la loss del batch\n",
    "    valid_epoch_loss = valid_epoch_loss + loss.item()\n",
    "\n",
    "  # Calculo la media de la loss\n",
    "  valid_epoch_loss = valid_epoch_loss / n_valid\n",
    "  # Almaceno la loss de la epoch para graficar\n",
    "  valid_loss_by_epoch.append(valid_epoch_loss)\n",
    "  # Cálculo la métrica de la epoch\n",
    "  valid_accuracy = metrics.r2_score(valid_epoch_y,valid_epoch_y_hat)\n",
    "\n",
    "  ############################################\n",
    "  ## Impresión de resultados por epoch\n",
    "  ############################################\n",
    "  print(f\" Epoch {epoch} | \" \\\n",
    "        f\"Train/Valid loss: {epoch_loss:.3f} / {valid_epoch_loss:.3f} | \" \\\n",
    "        f\"Train/Valid accuracy: {accuracy:.3f} / {valid_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1= plt.subplots(1,1)\n",
    "ax1.plot(train_loss_by_epoch, label=\"Train loss\")\n",
    "ax1.plot(valid_loss_by_epoch, label=\"Validation loss\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Loss by epoch\")\n",
    "ax1.set_xlabel(\"epoch\")\n",
    "ax1.set_ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data('../data/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "      <th>Outlet_Type_Grocery Store</th>\n",
       "      <th>Outlet_Type_Supermarket Type1</th>\n",
       "      <th>Outlet_Type_Supermarket Type2</th>\n",
       "      <th>Outlet_Type_Supermarket Type3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.300</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3735.1380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.920</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>443.4228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.500</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2097.2700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>732.3800</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>994.7052</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>6.865</td>\n",
       "      <td>0.056783</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2778.3834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>8.380</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>549.2850</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>10.600</td>\n",
       "      <td>0.035186</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193.1136</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>7.210</td>\n",
       "      <td>0.145221</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1845.5976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8522</th>\n",
       "      <td>14.800</td>\n",
       "      <td>0.044878</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>765.6700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8523 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Item_Weight  Item_Visibility  Item_MRP  Outlet_Establishment_Year   \n",
       "0           9.300         0.016047         4                         21  \\\n",
       "1           5.920         0.019278         1                         11   \n",
       "2          17.500         0.016760         2                         21   \n",
       "3          19.200         0.000000         3                         22   \n",
       "4           8.930         0.000000         1                         33   \n",
       "...           ...              ...       ...                        ...   \n",
       "8518        6.865         0.056783         4                         33   \n",
       "8519        8.380         0.046982         2                         18   \n",
       "8520       10.600         0.035186         1                         16   \n",
       "8521        7.210         0.145221         2                         11   \n",
       "8522       14.800         0.044878         1                         23   \n",
       "\n",
       "      Outlet_Size  Outlet_Location_Type  Item_Outlet_Sales   \n",
       "0               1                     2          3735.1380  \\\n",
       "1               1                     0           443.4228   \n",
       "2               1                     2          2097.2700   \n",
       "3               0                     0           732.3800   \n",
       "4               2                     0           994.7052   \n",
       "...           ...                   ...                ...   \n",
       "8518            2                     0          2778.3834   \n",
       "8519            0                     1           549.2850   \n",
       "8520            0                     1          1193.1136   \n",
       "8521            1                     0          1845.5976   \n",
       "8522            0                     2           765.6700   \n",
       "\n",
       "      Outlet_Type_Grocery Store  Outlet_Type_Supermarket Type1   \n",
       "0                             0                              1  \\\n",
       "1                             0                              0   \n",
       "2                             0                              1   \n",
       "3                             1                              0   \n",
       "4                             0                              1   \n",
       "...                         ...                            ...   \n",
       "8518                          0                              1   \n",
       "8519                          0                              1   \n",
       "8520                          0                              1   \n",
       "8521                          0                              0   \n",
       "8522                          0                              1   \n",
       "\n",
       "      Outlet_Type_Supermarket Type2  Outlet_Type_Supermarket Type3  \n",
       "0                                 0                              0  \n",
       "1                                 1                              0  \n",
       "2                                 0                              0  \n",
       "3                                 0                              0  \n",
       "4                                 0                              0  \n",
       "...                             ...                            ...  \n",
       "8518                              0                              0  \n",
       "8519                              0                              0  \n",
       "8520                              0                              0  \n",
       "8521                              1                              0  \n",
       "8522                              0                              0  \n",
       "\n",
       "[8523 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
